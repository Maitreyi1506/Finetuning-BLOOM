{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n!pip install transformers\n!pip install accelerate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nimport torch\nimport time, sys\nfrom torch.utils.data import DataLoader","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class chaiDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(tokenizer,data):\n    encodings = tokenizer(list(data[\"context\"]), list(data[\"question\"]), truncation=True, padding=True)\n    start_positions = []\n    end_positions = []\n    id_ = []\n\n    for i in range(len(data[\"answer_start\"])):\n        start_positions.append(encodings.char_to_token(i,data[\"answer_start\"][i]))\n        end_positions.append(encodings.char_to_token( i,(data[\"answer_start\"][i] + len(data['answer_text'][i]) - 1) ))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n\n\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\n    return encodings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = datasets.load_dataset(\"ai4bharat/IndicQA\", \"indicqa.hi\")\ndf = pd.DataFrame(dataset)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df,test_df = train_test_split(df test_size=0.25)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(data):\n    encodings = tokenizer(list(data[\"context\"]),list(data[\"question\"]), truncation=True, padding=True)\n    \n    start_positions = []\n    end_positions = []\n    for i in range(len(data[\"answer_start\"])):\n        start_positions.append(encodings.char_to_token(i,data[\"answer_start\"][i]))\n        end_positions.append(encodings.char_to_token( i, (data[\"answer_start\"][i] + len(data['answer_text'][i])-1) ))\n        \n            \n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n        \n        \n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n    \n    return encodings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = preprocess_data(train_df)\ntest_encodings = proprocess_data(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = chaiDataset(train_encodings)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = chaiDataset(test_encodings)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BloomForCausalLM.from_pretrained(\"bigscience/bloomz-560m\").to(\"cuda\")\n\nfor param in model.roberta.parameters():                #need to check the bloom version of this\n    param.requires_grad = False\n\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(), lr=1e-4)\nmodel.train()\n\nfor epoch in range(40):\n    epoch_loss=0\n    for batch in train_loader:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        batch_loss=loss.item()\n        optim.step()\n        epoch_loss+=batch_loss\n    normalized_epoch_loss = epoch_loss/(len(train_loader))\n    print(\"Epoch {} ; Epoch loss: {} \".format(epoch+1,normalized_epoch_loss))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_words,output_id = [],[]\nfor batch in test_loader:\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask)\n    start = torch.argmax(outputs[\"start_logits\"])\n    end = torch.argmax(outputs[\"end_logits\"])\n    output_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][start:end+1])\n    output_words.append(tokenizer.convert_tokens_to_string(output_tokens))","metadata":{},"execution_count":null,"outputs":[]}]}